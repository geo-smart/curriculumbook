{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering - Homework\n",
    "\n",
    "In the tutorial, we have seen how to choose the number of clusters using the elbow method. However, we have also noticed that it does not always work very well. Let us study another method based on the prediction strength.\n",
    "\n",
    "To know more about it, you can read the paper: Tibshirani, R. and Walther, G. (2005) Cluster validation by prediction strength. Journal of Computational and Graphical Statistics 14(3):511-528.\n",
    "\n",
    "In this homework, we are going to  use the function KMeans from the SciKitLearn package. See here for the documentation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction strength\n",
    "\n",
    "Suppose we have a dataset of $X = \\left\\{ x_{i , j} \\right\\}$ of $n$ observations of $d$-dimensional variables. Let us divide these observations into a train set $X_{tr}$ and a test set $X_{te}$ of size $n_{tr}$ and $n_{te}$ respectively.\n",
    "\n",
    "Let us choose the number of clusters $k$, and apply clustering to both the training data and the test data independently.\n",
    "\n",
    "Let us now denote $A_{1} , A_{2} , \\cdots , A_{k}$ the indices of the test observations in the test clusters $1 , 2 , \\cdots , k$, and $n_{1} , n_{2} , \\cdots , n_{k}$ the number of observations in these clusters.\n",
    "\n",
    "We now consider the clusters obtained with the training data, and denote this classifying operation $C \\left( X_{tr} \\right)$. We now apply this classifying operation to the test set. \n",
    "\n",
    "Let us now denote $D_j \\left[ C \\left( X_{tr} , k \\right) , X_{te} \\right]$ the $n_{te}$ by $n_{te}$ matrix which $i i'$ element $D_j \\left[ C \\left( X_{tr} , k \\right) , X_{te} \\right] _{i i'}$ is equal to $1$ if observations $i$ and $i'$ from the $j$th cluster of the test set fall into the same training set cluster, and $0$ otherwise. The prediction strength is then defined by:\n",
    "\n",
    "$ps \\left( k \\right) = \\min_{ 1 \\leq j \\leq k} \\frac{1}{n_{j} \\left( n_{j } - 1 \\right)} \\sum_{i \\neq i' \\in A_{j}} D_j \\left[ C \\left( X_{tr} , k \\right) , X_{te} \\right] _{i i'}$ (**eq 1**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import useful Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import cos, sin, pi, sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from the PNSN earthquake catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv('pnsn_catalog.csv')\n",
    "catalog.drop(columns=['Evid', 'Magnitude', 'Magnitude Type', 'Epoch(UTC)', 'Time UTC', 'Time Local', 'Distance From', 'Depth Mi'], inplace=True)\n",
    "catalog.columns = ['latitude', 'longitude', 'depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = catalog.to_numpy()\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "scaler = preprocessing.StandardScaler().fit(data_pca)\n",
    "data_scaled = scaler.transform(data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (1 point)\n",
    "\n",
    "Write code to divide the data into a training set and a test set of approximately the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (2 points)\n",
    "\n",
    "For now, we choose to have k = 2 clusters.\n",
    "\n",
    "Write code to apply K-means clustering to the training set and the test set using the Kmeans function from ScikitLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (2 points)\n",
    "\n",
    "Get the clusters for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data from the test set with two different colors for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (2 points)\n",
    "\n",
    "Use the clustering and centroids from the training set to predict to which cluster the data points from the test set should belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data from the test set with two different colors for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Compute the prediction strength for $k$ = 2 as defined at the beginning. Hint: use **eq 1** with nested loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Write a function that does steps 1 to 5 for any number $k$ of clusters and return the prediction strength or a given $k$ number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Apply this function to $k = 2, \\cdots , 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Plot the prediction strength as a function of number of clusters. What is the optimal number of clusters for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
